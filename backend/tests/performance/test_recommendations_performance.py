"""
æ¨èæ¥å£æ€§èƒ½æµ‹è¯•

ä¸“æ³¨äºæ€§èƒ½æŒ‡æ ‡æµ‹è¯•ï¼ŒåŒ…æ‹¬ï¼š
- å“åº”æ—¶é—´åŸºå‡†æµ‹è¯•
- ååé‡æµ‹è¯•
- å†…å­˜ä½¿ç”¨æµ‹è¯•
- å¹¶å‘å‹åŠ›æµ‹è¯•
"""

import pytest
import logging
import time
import concurrent.futures
import threading
from typing import List, Dict, Any
from statistics import mean, median, stdev

from tests.base_test import (
    FastAPITestClient, 
    RequestsTestClient,
    BaseRecommendationTest, 
    TestPerformanceMixin,
    TestDataGenerator
)

pytestmark = [pytest.mark.performance, pytest.mark.slow]

logger = logging.getLogger(__name__)


class TestRecommendationsPerformance(TestPerformanceMixin):
    """æ¨èæ¥å£æ€§èƒ½æµ‹è¯•ç±»"""
    
    def setup_method(self):
        """æ¯ä¸ªæµ‹è¯•æ–¹æ³•çš„åˆå§‹åŒ–"""
        # ä½¿ç”¨FastAPI TestClientä»¥è·å¾—æ›´ç¨³å®šçš„æ€§èƒ½æµ‹è¯•
        self.client = FastAPITestClient()
        self.api_test = BaseRecommendationTest(self.client)
        
        logger.info("âš¡ æ¨èæ¥å£æ€§èƒ½æµ‹è¯•å¼€å§‹")
        
        # è®¾ç½®æµ‹è¯•ç”¨æˆ·ï¼ˆæ€§èƒ½æµ‹è¯•ä½¿ç”¨å…±äº«ç”¨æˆ·ä»¥é¿å…é‡å¤åˆ›å»ºï¼‰
        self.user_info = self.api_test.setup_test_user()
        self.headers = self.user_info["headers"]  # ç›´æ¥ä½¿ç”¨è¿”å›çš„headers
        logger.info(f"âœ… æµ‹è¯•ç”¨æˆ·è®¾ç½®å®Œæˆ: {self.user_info['user']['username']}")
        
        # åˆ›å»ºåŸºç¡€æµ‹è¯•æ•°æ®
        self.test_cards = []
        card_data_list = TestDataGenerator.generate_test_cards(5)  # å‡å°‘æ•°é‡ä»¥æé«˜æ€§èƒ½
        for card_data in card_data_list:
            card = self.api_test.create_test_card(card_data)
            self.test_cards.append(card)
            
            # ä¸ºæ¯å¼ å¡åˆ›å»ºäº¤æ˜“è®°å½•
            transaction_data_list = TestDataGenerator.generate_test_transactions(card["id"], 10)
            for transaction_data in transaction_data_list:
                self.api_test.create_test_transaction(card["id"], transaction_data)
        
        logger.info(f"âœ… æ€§èƒ½æµ‹è¯•æ•°æ®åˆ›å»ºå®Œæˆ: {len(self.test_cards)}å¼ å¡")

    
    def _measure_multiple_requests(self, request_func, count: int = 100) -> Dict[str, Any]:
        """æµ‹é‡å¤šæ¬¡è¯·æ±‚çš„æ€§èƒ½æŒ‡æ ‡"""
        response_times = []
        success_count = 0
        error_count = 0
        
        start_time = time.time()
        
        for i in range(count):
            request_start = time.time()
            try:
                result = request_func()
                request_end = time.time()
                response_times.append(request_end - request_start)
                success_count += 1
            except Exception as e:
                request_end = time.time()
                response_times.append(request_end - request_start)
                error_count += 1
                logger.warning(f"è¯·æ±‚ {i+1} å¤±è´¥: {e}")
        
        end_time = time.time()
        total_time = end_time - start_time
        
        if response_times:
            return {
                "total_requests": count,
                "success_count": success_count,
                "error_count": error_count,
                "success_rate": success_count / count * 100,
                "total_time": total_time,
                "avg_response_time": mean(response_times),
                "median_response_time": median(response_times),
                "min_response_time": min(response_times),
                "max_response_time": max(response_times),
                "std_response_time": stdev(response_times) if len(response_times) > 1 else 0,
                "requests_per_second": count / total_time,
                "response_times": response_times
            }
        else:
            return {
                "total_requests": count,
                "success_count": 0,
                "error_count": count,
                "success_rate": 0,
                "total_time": total_time
            }
    
    def test_01_user_profile_stats_performance(self):
        """æµ‹è¯•ç”¨æˆ·ç”»åƒåˆ†ææ€§èƒ½"""
        logger.info("ğŸ“Š æµ‹è¯•ç”¨æˆ·ç”»åƒåˆ†ææ€§èƒ½...")
        
        def request_func():
            return self.api_test.test_user_profile_stats()
        
        # æµ‹é‡100æ¬¡è¯·æ±‚
        metrics = self._measure_multiple_requests(request_func, count=100)
        
        # æ€§èƒ½åŸºå‡†éªŒè¯
        assert metrics["success_rate"] >= 95, f"æˆåŠŸç‡è¿‡ä½: {metrics['success_rate']:.1f}%"
        assert metrics["avg_response_time"] < 0.5, f"å¹³å‡å“åº”æ—¶é—´è¿‡é•¿: {metrics['avg_response_time']:.3f}s"
        assert metrics["max_response_time"] < 2.0, f"æœ€å¤§å“åº”æ—¶é—´è¿‡é•¿: {metrics['max_response_time']:.3f}s"
        assert metrics["requests_per_second"] > 20, f"æ¯ç§’è¯·æ±‚æ•°è¿‡ä½: {metrics['requests_per_second']:.1f}"
        
        logger.info(f"âœ… ç”¨æˆ·ç”»åƒæ€§èƒ½éªŒè¯é€šè¿‡:")
        logger.info(f"   - å¹³å‡å“åº”æ—¶é—´: {metrics['avg_response_time']:.3f}s")
        logger.info(f"   - ä¸­ä½æ•°å“åº”æ—¶é—´: {metrics['median_response_time']:.3f}s")
        logger.info(f"   - æ¯ç§’å¤„ç†è¯·æ±‚: {metrics['requests_per_second']:.1f}")
        logger.info(f"   - æˆåŠŸç‡: {metrics['success_rate']:.1f}%")
    
    def test_02_generate_recommendations_performance(self):
        """æµ‹è¯•æ¨èç”Ÿæˆæ€§èƒ½"""
        logger.info("ğŸ¯ æµ‹è¯•æ¨èç”Ÿæˆæ€§èƒ½...")
        
        def request_func():
            return self.api_test.test_generate_recommendations()
        
        # æµ‹é‡50æ¬¡è¯·æ±‚ï¼ˆæ¨èç”Ÿæˆç›¸å¯¹è¾ƒæ…¢ï¼‰
        metrics = self._measure_multiple_requests(request_func, count=50)
        
        # æ€§èƒ½åŸºå‡†éªŒè¯
        assert metrics["success_rate"] >= 90, f"æˆåŠŸç‡è¿‡ä½: {metrics['success_rate']:.1f}%"
        assert metrics["avg_response_time"] < 2.0, f"å¹³å‡å“åº”æ—¶é—´è¿‡é•¿: {metrics['avg_response_time']:.3f}s"
        assert metrics["max_response_time"] < 5.0, f"æœ€å¤§å“åº”æ—¶é—´è¿‡é•¿: {metrics['max_response_time']:.3f}s"
        assert metrics["requests_per_second"] > 5, f"æ¯ç§’è¯·æ±‚æ•°è¿‡ä½: {metrics['requests_per_second']:.1f}"
        
        logger.info(f"âœ… æ¨èç”Ÿæˆæ€§èƒ½éªŒè¯é€šè¿‡:")
        logger.info(f"   - å¹³å‡å“åº”æ—¶é—´: {metrics['avg_response_time']:.3f}s")
        logger.info(f"   - ä¸­ä½æ•°å“åº”æ—¶é—´: {metrics['median_response_time']:.3f}s")
        logger.info(f"   - æ¯ç§’å¤„ç†è¯·æ±‚: {metrics['requests_per_second']:.1f}")
        logger.info(f"   - æˆåŠŸç‡: {metrics['success_rate']:.1f}%")
    
    def test_03_recommendations_list_performance(self):
        """æµ‹è¯•æ¨èåˆ—è¡¨æ€§èƒ½"""
        logger.info("ğŸ“„ æµ‹è¯•æ¨èåˆ—è¡¨æ€§èƒ½...")
        
        # å…ˆç”Ÿæˆä¸€äº›æ¨è
        self.api_test.test_generate_recommendations()
        
        def request_func():
            return self.api_test.test_get_recommendations_list(page_size=20)
        
        # æµ‹é‡100æ¬¡è¯·æ±‚
        metrics = self._measure_multiple_requests(request_func, count=100)
        
        # æ€§èƒ½åŸºå‡†éªŒè¯
        assert metrics["success_rate"] >= 95, f"æˆåŠŸç‡è¿‡ä½: {metrics['success_rate']:.1f}%"
        assert metrics["avg_response_time"] < 0.3, f"å¹³å‡å“åº”æ—¶é—´è¿‡é•¿: {metrics['avg_response_time']:.3f}s"
        assert metrics["max_response_time"] < 1.0, f"æœ€å¤§å“åº”æ—¶é—´è¿‡é•¿: {metrics['max_response_time']:.3f}s"
        assert metrics["requests_per_second"] > 30, f"æ¯ç§’è¯·æ±‚æ•°è¿‡ä½: {metrics['requests_per_second']:.1f}"
        
        logger.info(f"âœ… æ¨èåˆ—è¡¨æ€§èƒ½éªŒè¯é€šè¿‡:")
        logger.info(f"   - å¹³å‡å“åº”æ—¶é—´: {metrics['avg_response_time']:.3f}s")
        logger.info(f"   - ä¸­ä½æ•°å“åº”æ—¶é—´: {metrics['median_response_time']:.3f}s")
        logger.info(f"   - æ¯ç§’å¤„ç†è¯·æ±‚: {metrics['requests_per_second']:.1f}")
        logger.info(f"   - æˆåŠŸç‡: {metrics['success_rate']:.1f}%")
    
    def test_04_concurrent_requests_performance(self):
        """æµ‹è¯•å¹¶å‘è¯·æ±‚æ€§èƒ½"""
        logger.info("ğŸš€ æµ‹è¯•å¹¶å‘è¯·æ±‚æ€§èƒ½...")
        
        # å…ˆç”Ÿæˆä¸€äº›æ¨è
        self.api_test.test_generate_recommendations()
        
        def single_request(index):
            """å•ä¸ªå¹¶å‘è¯·æ±‚"""
            start_time = time.time()
            try:
                data = self.api_test.test_get_recommendations_list(page_size=10)
                end_time = time.time()
                return {
                    "index": index,
                    "success": True,
                    "response_time": end_time - start_time,
                    "item_count": len(data["items"])
                }
            except Exception as e:
                end_time = time.time()
                return {
                    "index": index,
                    "success": False,
                    "response_time": end_time - start_time,
                    "error": str(e)
                }
        
        # æµ‹è¯•ä¸åŒå¹¶å‘çº§åˆ«
        concurrency_levels = [5, 10, 20]
        
        for concurrency in concurrency_levels:
            logger.info(f"æµ‹è¯•å¹¶å‘çº§åˆ«: {concurrency}")
            
            start_time = time.time()
            
            with concurrent.futures.ThreadPoolExecutor(max_workers=concurrency) as executor:
                futures = [executor.submit(single_request, i) for i in range(concurrency * 2)]
                results = [future.result() for future in concurrent.futures.as_completed(futures)]
            
            end_time = time.time()
            total_time = end_time - start_time
            
            # åˆ†æå¹¶å‘æµ‹è¯•ç»“æœ
            successful_requests = [r for r in results if r["success"]]
            failed_requests = [r for r in results if not r["success"]]
            
            success_rate = len(successful_requests) / len(results) * 100
            avg_response_time = mean([r["response_time"] for r in successful_requests]) if successful_requests else 0
            requests_per_second = len(results) / total_time
            
            # éªŒè¯å¹¶å‘æ€§èƒ½
            assert success_rate >= 90, f"å¹¶å‘{concurrency}æˆåŠŸç‡è¿‡ä½: {success_rate:.1f}%"
            assert avg_response_time < 3.0, f"å¹¶å‘{concurrency}å¹³å‡å“åº”æ—¶é—´è¿‡é•¿: {avg_response_time:.3f}s"
            
            logger.info(f"âœ… å¹¶å‘{concurrency}æ€§èƒ½éªŒè¯é€šè¿‡: æˆåŠŸç‡{success_rate:.1f}%ï¼Œå“åº”æ—¶é—´{avg_response_time:.3f}s")
    
    def test_05_memory_usage_stability(self):
        """æµ‹è¯•å†…å­˜ä½¿ç”¨ç¨³å®šæ€§"""
        logger.info("ğŸ’¾ æµ‹è¯•å†…å­˜ä½¿ç”¨ç¨³å®šæ€§...")
        
        def memory_intensive_requests():
            """å†…å­˜å¯†é›†å‹è¯·æ±‚åºåˆ—"""
            results = []
            for i in range(50):
                try:
                    # ç”Ÿæˆæ¨èï¼ˆç›¸å¯¹å†…å­˜å¯†é›†ï¼‰
                    recommendations = self.api_test.test_generate_recommendations()
                    # è·å–åˆ—è¡¨
                    list_data = self.api_test.test_get_recommendations_list(page_size=50)
                    results.append({
                        "iteration": i,
                        "recommendations_count": len(recommendations),
                        "list_items_count": len(list_data["items"])
                    })
                except Exception as e:
                    logger.warning(f"å†…å­˜æµ‹è¯•è¿­ä»£ {i} å¤±è´¥: {e}")
            
            return results
        
        # æ‰§è¡Œå†…å­˜å¯†é›†å‹æµ‹è¯•
        start_time = time.time()
        results = memory_intensive_requests()
        end_time = time.time()
        
        # éªŒè¯ç»“æœ
        assert len(results) >= 45, f"å†…å­˜æµ‹è¯•å®Œæˆç‡è¿‡ä½: {len(results)}/50"
        
        total_time = end_time - start_time
        avg_time_per_iteration = total_time / len(results)
        
        logger.info(f"âœ… å†…å­˜ç¨³å®šæ€§éªŒè¯é€šè¿‡:")
        logger.info(f"   - å®Œæˆè¿­ä»£: {len(results)}/50")
        logger.info(f"   - æ€»è€—æ—¶: {total_time:.3f}s")
        logger.info(f"   - å¹³å‡æ¯æ¬¡è¿­ä»£: {avg_time_per_iteration:.3f}s")
    
    def test_06_pagination_performance_scaling(self):
        """æµ‹è¯•åˆ†é¡µæ€§èƒ½ä¼¸ç¼©æ€§"""
        logger.info("ğŸ“Š æµ‹è¯•åˆ†é¡µæ€§èƒ½ä¼¸ç¼©æ€§...")
        
        # å…ˆç”Ÿæˆä¸€äº›æ¨èæ•°æ®
        for _ in range(3):
            self.api_test.test_generate_recommendations()
        
        page_sizes = [10, 20, 50, 100]
        performance_data = []
        
        for page_size in page_sizes:
            def request_func():
                return self.api_test.test_get_recommendations_list(page_size=page_size)
            
            # æµ‹é‡æ¯ç§åˆ†é¡µå¤§å°çš„æ€§èƒ½
            metrics = self._measure_multiple_requests(request_func, count=20)
            
            performance_data.append({
                "page_size": page_size,
                "avg_response_time": metrics["avg_response_time"],
                "requests_per_second": metrics["requests_per_second"],
                "success_rate": metrics["success_rate"]
            })
            
            # éªŒè¯åŸºæœ¬æ€§èƒ½è¦æ±‚
            assert metrics["success_rate"] >= 95, f"åˆ†é¡µå¤§å°{page_size}æˆåŠŸç‡è¿‡ä½"
            assert metrics["avg_response_time"] < 1.0, f"åˆ†é¡µå¤§å°{page_size}å“åº”æ—¶é—´è¿‡é•¿"
            
            logger.info(f"âœ… åˆ†é¡µå¤§å°{page_size}æ€§èƒ½éªŒè¯é€šè¿‡: {metrics['avg_response_time']:.3f}s")
        
        # éªŒè¯å“åº”æ—¶é—´ä¸åˆ†é¡µå¤§å°çš„åˆç†å…³ç³»
        # å“åº”æ—¶é—´ä¸åº”è¯¥éšåˆ†é¡µå¤§å°çº¿æ€§å¢é•¿è¿‡å¿«
        min_time = min(p["avg_response_time"] for p in performance_data)
        max_time = max(p["avg_response_time"] for p in performance_data)
        time_ratio = max_time / min_time if min_time > 0 else 1
        
        assert time_ratio < 5.0, f"åˆ†é¡µå“åº”æ—¶é—´ä¼¸ç¼©æ€§è¿‡å·®: æœ€å¤§/æœ€å°æ—¶é—´æ¯”ä¾‹{time_ratio:.2f}"
        
        logger.info(f"âœ… åˆ†é¡µæ€§èƒ½ä¼¸ç¼©æ€§éªŒè¯é€šè¿‡: æ—¶é—´æ¯”ä¾‹{time_ratio:.2f}")
    
    def test_07_search_performance(self):
        """æµ‹è¯•æœç´¢æ€§èƒ½"""
        logger.info("ğŸ” æµ‹è¯•æœç´¢æ€§èƒ½...")
        
        # å…ˆç”Ÿæˆä¸€äº›æ¨èæ•°æ®
        self.api_test.test_generate_recommendations()
        
        search_keywords = ["ä¿¡ç”¨å¡", "é“¶è¡Œ", "æ¨è", "ä¼˜æƒ ", "ç§¯åˆ†"]
        
        for keyword in search_keywords:
            def request_func():
                return self.api_test.test_get_recommendations_list(keyword=keyword)
            
            # æµ‹é‡æœç´¢æ€§èƒ½
            metrics = self._measure_multiple_requests(request_func, count=30)
            
            # éªŒè¯æœç´¢æ€§èƒ½
            assert metrics["success_rate"] >= 90, f"æœç´¢'{keyword}'æˆåŠŸç‡è¿‡ä½: {metrics['success_rate']:.1f}%"
            assert metrics["avg_response_time"] < 0.8, f"æœç´¢'{keyword}'å“åº”æ—¶é—´è¿‡é•¿: {metrics['avg_response_time']:.3f}s"
            
            logger.info(f"âœ… æœç´¢'{keyword}'æ€§èƒ½éªŒè¯é€šè¿‡: {metrics['avg_response_time']:.3f}s")
    
    def test_08_stress_test(self):
        """å‹åŠ›æµ‹è¯•"""
        logger.info("ğŸ’ª æ‰§è¡Œå‹åŠ›æµ‹è¯•...")
        
        # å…ˆç”Ÿæˆä¸€äº›æ¨èæ•°æ®
        self.api_test.test_generate_recommendations()
        
        def stress_request(index):
            """å‹åŠ›æµ‹è¯•è¯·æ±‚"""
            operations = [
                lambda: self.api_test.test_user_profile_stats(),
                lambda: self.api_test.test_generate_recommendations(),
                lambda: self.api_test.test_get_recommendations_list(page_size=20),
                lambda: self.api_test.test_get_recommendations_list(keyword="ä¿¡ç”¨å¡")
            ]
            
            # éšæœºé€‰æ‹©æ“ä½œ
            import random
            operation = random.choice(operations)
            
            start_time = time.time()
            try:
                result = operation()
                end_time = time.time()
                return {
                    "index": index,
                    "success": True,
                    "response_time": end_time - start_time,
                    "operation": operation.__name__ if hasattr(operation, '__name__') else "lambda"
                }
            except Exception as e:
                end_time = time.time()
                return {
                    "index": index,
                    "success": False,
                    "response_time": end_time - start_time,
                    "error": str(e)
                }
        
        # æ‰§è¡Œå‹åŠ›æµ‹è¯•ï¼š100ä¸ªå¹¶å‘è¯·æ±‚
        start_time = time.time()
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
            futures = [executor.submit(stress_request, i) for i in range(100)]
            results = [future.result() for future in concurrent.futures.as_completed(futures)]
        
        end_time = time.time()
        total_time = end_time - start_time
        
        # åˆ†æå‹åŠ›æµ‹è¯•ç»“æœ
        successful_requests = [r for r in results if r["success"]]
        failed_requests = [r for r in results if not r["success"]]
        
        success_rate = len(successful_requests) / len(results) * 100
        avg_response_time = mean([r["response_time"] for r in successful_requests]) if successful_requests else 0
        requests_per_second = len(results) / total_time
        
        # éªŒè¯å‹åŠ›æµ‹è¯•ç»“æœ
        assert success_rate >= 85, f"å‹åŠ›æµ‹è¯•æˆåŠŸç‡è¿‡ä½: {success_rate:.1f}%"
        assert avg_response_time < 3.0, f"å‹åŠ›æµ‹è¯•å¹³å‡å“åº”æ—¶é—´è¿‡é•¿: {avg_response_time:.3f}s"
        
        logger.info(f"âœ… å‹åŠ›æµ‹è¯•éªŒè¯é€šè¿‡:")
        logger.info(f"   - æ€»è¯·æ±‚: {len(results)}")
        logger.info(f"   - æˆåŠŸç‡: {success_rate:.1f}%")
        logger.info(f"   - å¹³å‡å“åº”æ—¶é—´: {avg_response_time:.3f}s")
        logger.info(f"   - æ¯ç§’å¤„ç†è¯·æ±‚: {requests_per_second:.1f}")
        logger.info(f"   - æ€»è€—æ—¶: {total_time:.3f}s")
        
        if failed_requests:
            logger.warning(f"   - å¤±è´¥è¯·æ±‚: {len(failed_requests)}ä¸ª")
    
    def test_09_performance_consistency(self):
        """æµ‹è¯•æ€§èƒ½ä¸€è‡´æ€§"""
        logger.info("ğŸ“ˆ æµ‹è¯•æ€§èƒ½ä¸€è‡´æ€§...")
        
        # å¤šè½®æ€§èƒ½æµ‹è¯•ï¼Œæ£€æŸ¥æ€§èƒ½ä¸€è‡´æ€§
        rounds = 5
        round_results = []
        
        for round_num in range(rounds):
            logger.info(f"æ‰§è¡Œç¬¬ {round_num + 1}/{rounds} è½®æ€§èƒ½æµ‹è¯•...")
            
            def request_func():
                return self.api_test.test_get_recommendations_list(page_size=20)
            
            metrics = self._measure_multiple_requests(request_func, count=20)
            round_results.append(metrics)
            
            time.sleep(1)  # è½®æ¬¡é—´çŸ­æš‚ä¼‘æ¯
        
        # åˆ†ææ€§èƒ½ä¸€è‡´æ€§
        avg_response_times = [r["avg_response_time"] for r in round_results]
        requests_per_seconds = [r["requests_per_second"] for r in round_results]
        success_rates = [r["success_rate"] for r in round_results]
        
        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡çš„æ ‡å‡†å·®
        response_time_std = stdev(avg_response_times) if len(avg_response_times) > 1 else 0
        rps_std = stdev(requests_per_seconds) if len(requests_per_seconds) > 1 else 0
        
        # éªŒè¯æ€§èƒ½ä¸€è‡´æ€§
        avg_response_time_mean = mean(avg_response_times)
        rps_mean = mean(requests_per_seconds)
        
        # æ ‡å‡†å·®ä¸åº”è¯¥å¤ªå¤§ï¼ˆä¸è¶…è¿‡å‡å€¼çš„30%ï¼‰
        response_time_cv = response_time_std / avg_response_time_mean if avg_response_time_mean > 0 else 0
        rps_cv = rps_std / rps_mean if rps_mean > 0 else 0
        
        assert response_time_cv < 0.3, f"å“åº”æ—¶é—´å˜å¼‚ç³»æ•°è¿‡å¤§: {response_time_cv:.3f}"
        assert rps_cv < 0.3, f"æ¯ç§’è¯·æ±‚æ•°å˜å¼‚ç³»æ•°è¿‡å¤§: {rps_cv:.3f}"
        
        logger.info(f"âœ… æ€§èƒ½ä¸€è‡´æ€§éªŒè¯é€šè¿‡:")
        logger.info(f"   - å“åº”æ—¶é—´å˜å¼‚ç³»æ•°: {response_time_cv:.3f}")
        logger.info(f"   - æ¯ç§’è¯·æ±‚æ•°å˜å¼‚ç³»æ•°: {rps_cv:.3f}")
        logger.info(f"   - å¹³å‡å“åº”æ—¶é—´: {avg_response_time_mean:.3f}s (Â±{response_time_std:.3f})")
        logger.info(f"   - å¹³å‡æ¯ç§’è¯·æ±‚æ•°: {rps_mean:.1f} (Â±{rps_std:.1f})")
    
    def test_10_performance_benchmark_summary(self):
        """æ€§èƒ½åŸºå‡†æ€»ç»“æµ‹è¯•"""
        logger.info("ğŸ“‹ æ‰§è¡Œæ€§èƒ½åŸºå‡†æ€»ç»“...")
        
        # å®šä¹‰åŸºå‡†æµ‹è¯•åœºæ™¯
        benchmark_scenarios = [
            ("ç”¨æˆ·ç”»åƒ", lambda: self.api_test.test_user_profile_stats()),
            ("æ¨èç”Ÿæˆ", lambda: self.api_test.test_generate_recommendations()),
            ("æ¨èåˆ—è¡¨", lambda: self.api_test.test_get_recommendations_list()),
            ("æœç´¢åŠŸèƒ½", lambda: self.api_test.test_get_recommendations_list(keyword="ä¿¡ç”¨å¡"))
        ]
        
        benchmark_results = []
        
        for scenario_name, scenario_func in benchmark_scenarios:
            logger.info(f"åŸºå‡†æµ‹è¯•: {scenario_name}")
            
            # æµ‹é‡æ€§èƒ½
            metrics = self._measure_multiple_requests(scenario_func, count=30)
            
            benchmark_results.append({
                "scenario": scenario_name,
                "avg_response_time": metrics["avg_response_time"],
                "median_response_time": metrics["median_response_time"],
                "p95_response_time": sorted(metrics["response_times"])[int(len(metrics["response_times"]) * 0.95)],
                "requests_per_second": metrics["requests_per_second"],
                "success_rate": metrics["success_rate"]
            })
            
            logger.info(f"   {scenario_name}: {metrics['avg_response_time']:.3f}s, {metrics['requests_per_second']:.1f} RPS")
        
        # éªŒè¯æ•´ä½“æ€§èƒ½æ»¡è¶³åŸºå‡†è¦æ±‚
        for result in benchmark_results:
            scenario = result["scenario"]
            
            # åŸºæœ¬æˆåŠŸç‡è¦æ±‚
            assert result["success_rate"] >= 95, f"{scenario}æˆåŠŸç‡ä¸è¾¾æ ‡: {result['success_rate']:.1f}%"
            
            # æ ¹æ®åœºæ™¯ç±»å‹è®¾ç½®ä¸åŒçš„å“åº”æ—¶é—´è¦æ±‚
            if scenario == "ç”¨æˆ·ç”»åƒ":
                assert result["avg_response_time"] < 0.5, f"{scenario}å“åº”æ—¶é—´ä¸è¾¾æ ‡"
                assert result["requests_per_second"] > 20, f"{scenario}ååé‡ä¸è¾¾æ ‡"
            elif scenario == "æ¨èç”Ÿæˆ":
                assert result["avg_response_time"] < 2.0, f"{scenario}å“åº”æ—¶é—´ä¸è¾¾æ ‡"
                assert result["requests_per_second"] > 5, f"{scenario}ååé‡ä¸è¾¾æ ‡"
            elif scenario == "æ¨èåˆ—è¡¨":
                assert result["avg_response_time"] < 0.3, f"{scenario}å“åº”æ—¶é—´ä¸è¾¾æ ‡"
                assert result["requests_per_second"] > 30, f"{scenario}ååé‡ä¸è¾¾æ ‡"
            elif scenario == "æœç´¢åŠŸèƒ½":
                assert result["avg_response_time"] < 0.8, f"{scenario}å“åº”æ—¶é—´ä¸è¾¾æ ‡"
                assert result["requests_per_second"] > 15, f"{scenario}ååé‡ä¸è¾¾æ ‡"
        
        logger.info("âœ… æ€§èƒ½åŸºå‡†æ€»ç»“éªŒè¯é€šè¿‡:")
        for result in benchmark_results:
            logger.info(f"   {result['scenario']}: "
                       f"å¹³å‡{result['avg_response_time']:.3f}s, "
                       f"P95 {result['p95_response_time']:.3f}s, "
                       f"{result['requests_per_second']:.1f} RPS") 